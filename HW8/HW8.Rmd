---
output:
  pdf_document: default
  html_document: default
---
# Duncan McKinnon
# West

# HW 8


```{r warning=FALSE, message=FALSE}
# Load R Packages
suppressPackageStartupMessages({
  library(tidyverse)
  library(NHANES)
  library(class)
  library(mosaic)
})
```

## QA

### 1,2,3).

```{r}
# create 1, 0 factors for RegularMarij and Gender, limit data to 11 variables
dNH <- NHANES %>%
  mutate(RM1 = ifelse(RegularMarij=='Yes', 1, 0), Sex1 = ifelse(Gender=='male', 1, 0)) %>%
  select(HardDrugs, RM1, Age, AlcoholYear, BMI, Sex1, HHIncomeMid, Weight, Height, SexNumPartnLife, TotChol)

# summarize new fields
dNH %>% select(RM1, Sex1) %>% summary()

dNH %>% glimpse()

# remove all NA's and show num left
dNH <- dNH %>% na.omit()

m <- dim(dNH)
m[1]
```

### 4).

```{r}
# set random seed
set.seed(1847)

# create function for splitting out data into train and test
split_data <- function(data, m, test_p) {
  idata <- list()
  idata$train_indices <- sample.int(m, (1 - test_p) * m)
  idata$train <- data[idata$train_indices, ]
  idata$test <- data[-idata$train_indices, ]
  return(idata)
}

# create train and test data
DNH <- split_data(dNH, m[1], 0.2)

# glimpse training and tests sets
glimpse(DNH$train)
glimpse(DNH$test)
```

### 5 a-d).

```{r}

# create function to re-run knn on the same data for many values of k
# return : list() with list() of the model test result values 
#                      list() of the confusion matrices and prediction accuracy of each model
reKNN <- function(dtrain, dtest, train_labels, test_labels, ks){
  mods <- list()
  confs <- list()
  pcts <- list()
  
  for(i in ks){
    # re-run the modeling step on each value of k and generate confusion matrix
    model_predictions <- knn(train = dtrain, test = dtest, cl = train_labels, k = i)
    confusion_matrix <- table(test_labels, model_predictions)
    
    # get model accuracy from the confusion matrix
    acc_pct <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / sum(confusion_matrix)
    
    # store results of modeling and confusion matrix/accuracy in list labeled by k used in model
    mods[[paste(i)]] <- model_predictions
    confs[[paste(i)]] <- list( 'confusion_matrix' = confusion_matrix, 'accuracy' = acc_pct )
  }
  return(list('model' = mods, 'performance' = confs))
}

# run reKNN for values of k
dtrn <- DNH$train %>% select(-HardDrugs)
dcltrn <- as.factor(DNH$train$HardDrugs)
dtst <- DNH$test %>% select(-HardDrugs)
dcltst <- as.factor(DNH$test$HardDrugs)
knns <- c(1, 5, 25, 100)

iknn <- reKNN(dtrn, dtst, dcltrn, dcltst, knns)

iknn$performance
```

### 6).

In the test of different k values (1,5,25,100) I found that k = 1 produced the most accurate predictions on this data set.  The model produced with k=1 was almost 88% accurate in predictions on the test set, while k=5, 25 and 100 were all less than 80% accurate.  This implies that the data may be so mixed in N dimensional space that only the most immediate neighbors are reasonable predictors of the overall class of the observation.

## QB

### 1, 2).

```{r}
# set random seed
set.seed(1847)

# create null model data
nullmod <- data.frame( y = sample(c(1,0), 500, replace = T), 
                       x1 = runif(500, 0, 10),
                       x2 = runif(500, 0, 10))
glimpse(nullmod)


# set random seed
set.seed(1847)

# create train and test data
p <- 0.2
m <- 500

train_indices_null <- sample.int(m, (1-p)*m)
train_null <- nullmod[train_indices_null, ]
test_null <- nullmod[-train_indices_null, ]

glimpse(train_null)

```

### 3).

```{r}
# create grid data
grid_null <- expand.grid(x1 = seq(0, 10, 0.1), x2 = seq(0, 10, 0.1))

glimpse(grid_null)
```


### 4, 5, 6, (7)).

```{r}

# split training data and labels
trn <- train_null %>% select(-y)
trn_labels <- factor(train_null$y)
kvs <- c(25,5,1,400)

# re-run the modeling step on each value of k and save the predictions to a list
mods <- list()
for(i in kvs){
  model_predictions <- knn(train = trn, test = grid_null, cl = trn_labels, k = i)
  mods[[i]] <- model_predictions
}

# plot the colors in the color grid using each set of model predictions
p <- ggplot(grid_null) + geom_point(aes(x = x1, y=x2))
p + aes( color = mods[[25]] ) + labs(title = 'knn predictions for k = 25', color = 'k = 25')
p + aes( color = mods[[5]] ) + labs(title = 'knn predictions for k = 5',color = 'k = 5')
p + aes( color = mods[[1]] ) + labs(title = 'knn predictions for k = 1',color = 'k = 1')

```

### 7).

The plot using k = n will always be uninformative because every prediction for each observation will just be an aggregation of the labels of every point in the training data, since the nearest k = n training points will include all n points in the training data.

```{r}
# plot the grid predictions for k = 400
p + aes( color = mods[[400]] ) + labs(title = 'knn predictions for k = 400',color = 'k = 400')
```


## QC

### 1).

```{r}
# set random seed
set.seed(1847)

# create x's and y in data frame
x1 <- c( rnorm(250, 6, 1), rnorm(250, 5, 1) )
x2 <- c( rnorm(250, 6, 1), rnorm(250, 5, 1) )
y <- as.factor(c( rep(1, 250), rep(0, 250) ))
dat_norm <- cbind.data.frame(x1, x2, y)

# plot data form
ggplot(dat_norm) + geom_point(aes(x = x1, y = x2, color = y))
```

### 2).

```{r}
# set random seed
set.seed(1847)
m <- 500
p <- 0.2

# split training and test set
indices_norm <- sample.int(m, (1-p)*m)
train_norm <- dat_norm[indices_norm, ]
test_norm <- dat_norm[-indices_norm, ]

# show training data
glimpse(train_norm)
```

### 3, 4, 5).

```{r}
# split training data and labels
trn <- train_norm %>% select(-y)
trn_labels <- factor(train_norm$y)
kvs <- c(25,5,1)

# re-run the modeling step on each value of k and save the predictions to a list
mods <- list()
for(i in kvs){
  model_predictions <- knn(train = trn, test = grid_null, cl = trn_labels, k = i)
  mods[[i]] <- model_predictions
}

# plot the colors in the color grid using each set of model predictions
p <- ggplot(grid_null) + geom_point(aes(x = x1, y=x2))
p + aes( color = mods[[25]] ) + labs(title = 'knn predictions for k = 25', color = 'k = 25')
p + aes( color = mods[[5]] ) + labs(title = 'knn predictions for k = 5',color = 'k = 5')
p + aes( color = mods[[1]] ) + labs(title = 'knn predictions for k = 1',color = 'k = 1')


```

### 6). 

As the values of k changed from 25 to 5 to 1 in the knn analysis, the influence of only the closest observations from the training data increased, causing the values close to the boundary to be increasingly influenced by the random mixing of the two labeled distributions in the data.  At higher values of k, a greater portion of the distribution was accounted for in labeling each observation and so more information capturing the overall form of the distribution was integrated into the decision making process.  

### 7).

It seems that the classifications for k = 25 more closely matched the boundary we would expect given the distributions used to create the training data. While k = 5 produced similar classifications at the extremes, the classifications closer to the boundary were clearly influenced more by noise in the distributions, as there were small disconnected pockets embedded across the boundary between distributions.  At n = 1, values at the extremes also started looking random, as outlying examples from the training set exhibited hi influence over classifications in regions where there were few examples to be near.


